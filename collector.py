#!/usr/bin/env python3
"""
AI News Collector
æ¯æ—¥ AM 2:00 JST ã«å„ãƒ–ãƒ­ã‚°/ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã—ã€
Claude APIã§ã‚µãƒãƒªã‚’ç”Ÿæˆã—ã¦Notionã«ä¿å­˜ã™ã‚‹ã€‚
"""

import os
import json
import hashlib
import logging
import re
import feedparser
import anthropic
import requests
from datetime import datetime, timezone, timedelta
from pathlib import Path
from dotenv import load_dotenv

# .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•èª­ã¿è¾¼ã¿ï¼ˆexport ã‚³ãƒãƒ³ãƒ‰ä¸è¦ï¼‰
load_dotenv(Path(__file__).parent / ".env")

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("collector.log"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)

JST = timezone(timedelta(hours=9))

# ================== è¨­å®š ==================

SOURCES = [
    {
        "name": "Anthropic Blog",
        # å…¬å¼RSSã¯å»ƒæ­¢æ¸ˆã¿ â†’ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ç®¡ç†ã®ãƒŸãƒ©ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
        "feed_url": "https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_anthropic_news.xml",
        "tag": "Anthropic",
        # ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ»è£½å“ãƒ»äº‹ä¾‹é–¢é€£ã«çµã‚Šè¾¼ã‚€
        "filter_keywords": [
            "enterprise", "agent", "Claude", "API", "deployment", "business",
            "partner", "case study", "customers", "tools", "model",
        ],
    },
    {
        "name": "OpenAI Blog",
        "feed_url": "https://openai.com/blog/rss.xml",
        "tag": "OpenAI",
        # ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ»è£½å“ãƒ»äº‹ä¾‹é–¢é€£ã«çµã‚Šè¾¼ã‚€
        "filter_keywords": [
            "enterprise", "agent", "GPT", "API", "deployment", "business",
            "partner", "case study", "customers", "o1", "o3",
        ],
    },
    {
        "name": "Google DeepMind Blog",
        "feed_url": "https://deepmind.google/blog/rss.xml",
        "tag": "Google",
        # ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ»è£½å“ãƒ»äº‹ä¾‹é–¢é€£ã«çµã‚Šè¾¼ã‚€
        "filter_keywords": [
            "enterprise", "agent", "Gemini", "API", "deployment", "business",
            "partner", "Vertex", "application", "product",
        ],
    },
    {
        "name": "a16z Newsletter",
        # å…¬å¼RSSã¯å»ƒæ­¢æ¸ˆã¿ â†’ Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—
        "feed_url": None,
        "scrape_url": "https://a16z.com/news-content/",
        "tag": "a16z",
        "filter_keywords": ["AI", "LLM", "machine learning", "foundation model", "artificial intelligence", "agent"],
    },
    {
        "name": "Salesforce Engineering Blog",
        "feed_url": "https://engineering.salesforce.com/feed/",
        "tag": "Salesforce",
        # AIãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–¢é€£è¨˜äº‹ã«çµã‚Šè¾¼ã‚€
        "filter_keywords": ["AI", "LLM", "Agentforce", "machine learning", "agent", "artificial intelligence"],
    },
    {
        "name": "Salesforce Blog",
        "feed_url": "https://www.salesforce.com/blog/feed/",
        "tag": "Salesforce",
        # AIãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–¢é€£è¨˜äº‹ã«çµã‚Šè¾¼ã‚€ï¼ˆéAIè¨˜äº‹ãŒå¤šã„ãŸã‚ãƒ•ã‚£ãƒ«ã‚¿å¿…é ˆï¼‰
        "filter_keywords": ["AI", "LLM", "Agentforce", "machine learning", "agent", "artificial intelligence"],
    },
    {
        "name": "VentureBeat AI",
        # ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºAIå°å…¥äº‹ä¾‹ãƒ»CXOæˆ¦ç•¥ãƒ»è£½å“ç™ºè¡¨ã«ç‰¹åŒ–ã—ãŸãƒ¡ãƒ‡ã‚£ã‚¢
        "feed_url": "https://venturebeat.com/category/ai/feed/",
        "tag": "VentureBeat",
        "filter_keywords": [
            "enterprise", "agent", "agentic", "deployment", "adoption", "case study",
            "ROI", "implementation", "CTO", "CEO", "strategy", "Salesforce", "Microsoft",
            "Google", "AWS", "SAP", "ServiceNow", "workflow", "automation",
        ],
    },
]

# æ—¢å‡¦ç†æ¸ˆã¿è¨˜äº‹ã®IDã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«
SEEN_IDS_FILE = Path(__file__).parent / "seen_ids.json"


# ================== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==================

def load_seen_ids() -> set:
    if SEEN_IDS_FILE.exists():
        with open(SEEN_IDS_FILE) as f:
            return set(json.load(f))
    return set()


def save_seen_ids(seen: set):
    with open(SEEN_IDS_FILE, "w") as f:
        json.dump(list(seen), f, indent=2)


def article_id(url: str) -> str:
    return hashlib.md5(url.encode()).hexdigest()


def is_recent(entry, hours: int = 72) -> bool:
    """published ã¾ãŸã¯ updated ãŒç›´è¿‘ hours æ™‚é–“ä»¥å†…ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ72hï¼‰"""
    for attr in ("published_parsed", "updated_parsed"):
        t = getattr(entry, attr, None)
        if t:
            import time
            dt = datetime.fromtimestamp(time.mktime(t), tz=timezone.utc)
            return datetime.now(timezone.utc) - dt < timedelta(hours=hours)
    return True  # æ—¥ä»˜ä¸æ˜ãªå ´åˆã¯å‡¦ç†å¯¾è±¡ã¨ã™ã‚‹


# ================== ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾— ==================

def fetch_a16z_scrape(source: dict, seen_ids: set) -> list[dict]:
    """a16z ã¯RSSãŒå»ƒæ­¢ã•ã‚Œã¦ã„ã‚‹ãŸã‚Webãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    logger.info(f"Scraping: {source['name']}")
    try:
        resp = requests.get(
            source["scrape_url"],
            headers={"User-Agent": "Mozilla/5.0 (compatible; AI-News-Collector/1.0)"},
            timeout=20,
        )
        resp.raise_for_status()
    except Exception as e:
        logger.error(f"Scrape error [{source['name']}]: {e}")
        return []

    # href="/..." ã®å½¢å¼ã®ãƒªãƒ³ã‚¯ã‚’ã™ã¹ã¦æŠ½å‡º
    links = re.findall(r'href="(https://a16z\.com/[^"]+)"', resp.text)
    # ã‚¿ã‚¤ãƒˆãƒ«å€™è£œï¼ˆaã‚¿ã‚°ã®ä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
    title_map = {}
    for m in re.finditer(r'href="(https://a16z\.com/[^"]+)"[^>]*>([^<]{10,120})<', resp.text):
        url, title = m.group(1), m.group(2).strip()
        if url not in title_map and title:
            title_map[url] = title

    filter_kw = [k.lower() for k in source.get("filter_keywords", [])]
    seen_urls: set = set()
    articles = []

    for url in links:
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¢ãƒ³ã‚«ãƒ¼ãƒªãƒ³ã‚¯ç­‰ã‚’é™¤å¤–
        if any(x in url for x in ["#", "?page=", "/podcast", "/video", "/category"]):
            continue
        if url in seen_urls:
            continue
        seen_urls.add(url)

        aid = article_id(url)
        if aid in seen_ids:
            continue

        title = title_map.get(url, url.split("/")[-2].replace("-", " ").title())

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
        if filter_kw and not any(kw in title.lower() for kw in filter_kw):
            continue

        articles.append({
            "id": aid,
            "source": source["name"],
            "tag": source["tag"],
            "title": title[:200],
            "url": url,
            "raw_summary": f"a16z ã®è¨˜äº‹: {title}",
            "published": "",
        })

        if len(articles) >= 10:  # ä¸€åº¦ã«æœ€å¤§10ä»¶
            break

    logger.info(f"  -> {len(articles)} new article(s) from {source['name']}")
    return articles


def fetch_feed(source: dict, seen_ids: set) -> list[dict]:
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã€æœªå‡¦ç†ã®æ–°ç€è¨˜äº‹ã‚’è¿”ã™"""
    # feed_url ãŒ None ã®ã‚½ãƒ¼ã‚¹ã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—
    if source.get("feed_url") is None:
        return fetch_a16z_scrape(source, seen_ids)

    logger.info(f"Fetching: {source['name']}")
    try:
        feed = feedparser.parse(source["feed_url"])
    except Exception as e:
        logger.error(f"Feed fetch error [{source['name']}]: {e}")
        return []

    articles = []
    filter_kw = [k.lower() for k in source.get("filter_keywords", [])]

    for entry in feed.entries:
        url = entry.get("link", "")
        if not url:
            continue

        aid = article_id(url)
        if aid in seen_ids:
            continue

        if not is_recent(entry):
            continue

        title = entry.get("title", "(no title)")
        summary_raw = entry.get("summary", entry.get("description", ""))

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
        if filter_kw:
            text = (title + " " + summary_raw).lower()
            if not any(kw in text for kw in filter_kw):
                continue

        articles.append({
            "id": aid,
            "source": source["name"],
            "tag": source["tag"],
            "title": title,
            "url": url,
            "raw_summary": summary_raw[:3000],  # Claude ã«æ¸¡ã™ä¸Šé™
            "published": entry.get("published", ""),
        })

    logger.info(f"  -> {len(articles)} new article(s) from {source['name']}")
    return articles


# ================== Claude ã‚µãƒãƒª ==================

def summarize(article: dict) -> str:
    """Claude API ã§è¨˜äº‹ã®ã‚µãƒãƒªã‚’æ—¥æœ¬èªã§ç”Ÿæˆã™ã‚‹"""
    client = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])

    prompt = f"""ä»¥ä¸‹ã®AIé–¢é€£è¨˜äº‹ã‚’æ—¥æœ¬èªã§ã‚µãƒãƒªã—ã¦ãã ã•ã„ã€‚
ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºAIå°å…¥ãƒ»æ´»ç”¨ã®è¦³ç‚¹ã‹ã‚‰é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}
URL: {article['url']}
æœ¬æ–‡æŠœç²‹:
{article['raw_summary']}

å‡ºåŠ›å½¢å¼ï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ï¼‰:
## æ¦‚è¦
ï¼ˆ2ã€œ3æ–‡ã§è¦ç‚¹ã‚’èª¬æ˜ï¼‰

## ãƒ“ã‚¸ãƒã‚¹ã¸ã®ç¤ºå”†
- é–¢é€£ã™ã‚‹æ¥­ç¨®ãƒ»éƒ¨é–€ï¼ˆä¾‹ï¼šå–¶æ¥­ã€ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã€è£½é€ æ¥­ãªã©ï¼‰
- å°å…¥ãƒ»æ´»ç”¨ã®ãƒã‚¤ãƒ³ãƒˆï¼ˆç®‡æ¡æ›¸ã 2ã€œ4é …ç›®ï¼‰

## é‡è¦åº¦
ï¼ˆHigh / Medium / Low ã¨ãã®ç†ç”±ã‚’1æ–‡ã§ï¼‰
"""

    try:
        message = client.messages.create(
            model="claude-haiku-4-5",
            max_tokens=600,
            messages=[{"role": "user", "content": prompt}],
        )
        return message.content[0].text
    except Exception as e:
        logger.error(f"Claude API error: {e}")
        return f"ã‚µãƒãƒªç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"


# ================== Notion ä¿å­˜ ==================

def notion_headers() -> dict:
    key = os.environ['NOTION_API_KEY']
    logger.debug(f"[DEBUG] NOTION_API_KEY in use: {key[:15]}...{key[-4:]} (len={len(key)})")
    return {
        "Authorization": f"Bearer {key}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28",
    }


def create_notion_page(article: dict, summary: str):
    """Notion ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«1è¨˜äº‹ = 1ãƒšãƒ¼ã‚¸ã‚’ä½œæˆã™ã‚‹"""
    database_id = os.environ["NOTION_DATABASE_ID"]

    # published ã‚’æ•´å½¢
    pub_str = article.get("published", "")
    published_date = None
    if pub_str:
        for fmt in ("%a, %d %b %Y %H:%M:%S %z", "%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%dT%H:%M:%SZ"):
            try:
                dt = datetime.strptime(pub_str, fmt)
                published_date = dt.astimezone(JST).strftime("%Y-%m-%d")
                break
            except ValueError:
                continue

    # Notionãƒšãƒ¼ã‚¸ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ï¼ˆDBã‚¹ã‚­ãƒ¼ãƒã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
    properties = {
        "Name": {
            "title": [{"text": {"content": article["title"][:200]}}]
        },
        "Source": {
            "select": {"name": article["source"]}
        },
        "Tag": {
            "select": {"name": article["tag"]}
        },
        "URL": {
            "url": article["url"]
        },
    }
    if published_date:
        properties["Published"] = {"date": {"start": published_date}}

    # ãƒšãƒ¼ã‚¸æœ¬æ–‡ï¼ˆã‚µãƒãƒªï¼‰
    body_blocks = []
    for line in summary.split("\n"):
        line = line.strip()
        if not line:
            continue
        if line.startswith("## "):
            body_blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {"rich_text": [{"type": "text", "text": {"content": line[3:]}}]},
            })
        elif line.startswith("- "):
            body_blocks.append({
                "object": "block",
                "type": "bulleted_list_item",
                "bulleted_list_item": {"rich_text": [{"type": "text", "text": {"content": line[2:]}}]},
            })
        else:
            body_blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": [{"type": "text", "text": {"content": line}}]},
            })

    payload = {
        "parent": {"database_id": database_id},
        "properties": properties,
        "children": body_blocks[:100],  # Notion API ã¯ä¸€åº¦ã«æœ€å¤§100ãƒ–ãƒ­ãƒƒã‚¯
    }

    resp = requests.post(
        "https://api.notion.com/v1/pages",
        headers=notion_headers(),
        json=payload,
        timeout=30,
    )
    if resp.status_code not in (200, 201):
        logger.error(f"Notion API error [{resp.status_code}]: {resp.text}")
        resp.raise_for_status()

    logger.info(f"  -> Notion page created: {article['title'][:60]}")


# ================== Podcast ç”Ÿæˆ ==================

def generate_podcast_script(articles: list) -> str:
    """åé›†ã—ãŸå…¨è¨˜äº‹ã‹ã‚‰ãƒ©ã‚¸ã‚ªç•ªçµ„é¢¨ã®æ—¥æœ¬èªã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
    client = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])

    articles_text = "\n\n".join([
        f"ã€{a['source']}ã€‘{a['title']}\n{a.get('summary', a.get('raw_summary', ''))[:500]}"
        for a in articles
    ])

    prompt = f"""ä»¥ä¸‹ã®æœ¬æ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã‚‚ã¨ã«ã€çµŒå–¶è€…ãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒªãƒ¼ãƒ€ãƒ¼å‘ã‘ã®ãƒ©ã‚¸ã‚ªç•ªçµ„é¢¨æ—¥æœ¬èªãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

{articles_text}

è¦ä»¶ï¼š
- å†’é ­ã¯ã€ŒãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€‚ä»Šæ—¥ã®ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚ã€ã§å§‹ã‚ã‚‹
- ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ»å°å…¥äº‹ä¾‹ãƒ»æ¥­ç•Œå‹•å‘ã‚’ä¸­å¿ƒã«è§£èª¬ã™ã‚‹
- çµŒå–¶è€…ãƒ»CxOè¦–ç‚¹ã§ã€Œè‡ªç¤¾ã§ã¯ã©ã†æ´»ç”¨ã§ãã‚‹ã‹ã€ã¨ã„ã†è¦–ç‚¹ã‚’æ„è­˜ã™ã‚‹
- å„è¨˜äº‹ã‚’è‡ªç„¶ãªãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¤ãªãï¼ˆã€Œç¶šã„ã¦ã€ã€Œã¾ãŸã€ã€Œæ¬¡ã«ã€ãªã©ã®æ¥ç¶šè©ã‚’ä½¿ã†ï¼‰
- å°‚é–€ç”¨èªã¯ã‚ã‹ã‚Šã‚„ã™ãè¨€ã„æ›ãˆã‚‹
- å…¨ä½“ã§3ã€œ4åˆ†ç¨‹åº¦ï¼ˆç´„900ã€œ1200æ–‡å­—ï¼‰
- ç· ã‚ã¯ã€Œä»¥ä¸Šã€ä»Šæ—¥ã®ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã—ãŸã€‚ã¾ãŸæ˜æ—¥ã€‚ã€ã§çµ‚ãˆã‚‹
- ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ã¯ä½¿ã‚ãšã€èª­ã¿ä¸Šã’ã«é©ã—ãŸãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§å‡ºåŠ›ã™ã‚‹
"""

    message = client.messages.create(
        model="claude-haiku-4-5",
        max_tokens=1500,
        messages=[{"role": "user", "content": prompt}],
    )
    return message.content[0].text


def create_podcast_mp3(script: str) -> bytes:
    """OpenAI TTS APIã§ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’MP3ã«å¤‰æ›ã™ã‚‹"""
    from openai import OpenAI
    client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

    response = client.audio.speech.create(
        model="tts-1",
        voice="nova",
        input=script,
        response_format="mp3",
    )
    return response.content


def save_podcast_to_notion(script: str, date_str: str):
    """Notionã«æœ¬æ—¥ã®Podcastã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒšãƒ¼ã‚¸ã‚’ä½œæˆã™ã‚‹"""
    database_id = os.environ["NOTION_DATABASE_ID"]

    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æ®µè½ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²
    blocks = []
    for para in script.split("\n"):
        para = para.strip()
        if not para:
            continue
        # 1ãƒ–ãƒ­ãƒƒã‚¯ã‚ãŸã‚Š2000æ–‡å­—åˆ¶é™
        while len(para) > 2000:
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": [{"type": "text", "text": {"content": para[:2000]}}]},
            })
            para = para[2000:]
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": [{"type": "text", "text": {"content": para}}]},
        })

    payload = {
        "parent": {"database_id": database_id},
        "properties": {
            "Name": {"title": [{"text": {"content": f"ğŸ“» {date_str} AI News Podcast"}}]},
            "Source": {"select": {"name": "Podcast"}},
            "Tag": {"select": {"name": "Podcast"}},
            "URL": {"url": "https://github.com"},
            "Published": {"date": {"start": date_str}},
        },
        "children": blocks[:100],
    }

    resp = requests.post(
        "https://api.notion.com/v1/pages",
        headers=notion_headers(),
        json=payload,
        timeout=30,
    )
    if resp.status_code not in (200, 201):
        logger.error(f"Notion Podcast page error [{resp.status_code}]: {resp.text}")
        resp.raise_for_status()

    logger.info(f"  -> Notion Podcast page created: {date_str}")


# ================== ãƒ¡ã‚¤ãƒ³ ==================

def main():
    logger.info("=== AI News Collector started ===")
    logger.info(f"Time (JST): {datetime.now(JST).strftime('%Y-%m-%d %H:%M:%S')}")

    # å¿…é ˆç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    for key in ("ANTHROPIC_API_KEY", "NOTION_API_KEY", "NOTION_DATABASE_ID"):
        if not os.environ.get(key):
            raise EnvironmentError(f"ç’°å¢ƒå¤‰æ•° {key} ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    seen_ids = load_seen_ids()
    new_seen: set = set()
    all_articles: list = []  # Podcastç”¨ã«å…¨è¨˜äº‹ã‚’è“„ç©

    for source in SOURCES:
        articles = fetch_feed(source, seen_ids)
        for article in articles:
            try:
                logger.info(f"Summarizing: {article['title'][:60]}")
                summary = summarize(article)
                create_notion_page(article, summary)
                article["summary"] = summary
                all_articles.append(article)
                new_seen.add(article["id"])
            except Exception as e:
                logger.error(f"Failed to process [{article['title'][:40]}]: {e}")

    seen_ids |= new_seen
    save_seen_ids(seen_ids)

    logger.info(f"=== Done. {len(new_seen)} article(s) processed ===")

    # æ–°è¦è¨˜äº‹ãŒã‚ã£ãŸæ—¥ã ã‘Podcastã‚’ç”Ÿæˆ
    if all_articles and os.environ.get("OPENAI_API_KEY"):
        logger.info("=== Generating Podcast ===")
        try:
            date_str = datetime.now(JST).strftime("%Y-%m-%d")
            script = generate_podcast_script(all_articles)
            mp3_data = create_podcast_mp3(script)
            mp3_path = Path(__file__).parent / "docs" / f"podcast_{date_str}.mp3"
            mp3_path.parent.mkdir(parents=True, exist_ok=True)
            mp3_path.write_bytes(mp3_data)
            logger.info(f"MP3 saved: {mp3_path.name} ({len(mp3_data)//1024}KB)")
            save_podcast_to_notion(script, date_str)
        except Exception as e:
            logger.error(f"Podcast generation failed: {e}")
    elif not os.environ.get("OPENAI_API_KEY"):
        logger.info("OPENAI_API_KEY not set, skipping Podcast generation")


if __name__ == "__main__":
    main()
